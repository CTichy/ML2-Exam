Question,Option,IsCorrect,Explanation
What is the primary goal of regularization in regression models?,To improve training accuracy,False,Regularization typically reduces training accuracy slightly to gain better generalization.
What is the primary goal of regularization in regression models?,To reduce model variance and improve generalization,True,Regularization reduces overfitting by penalizing complexity and lowering variance.
What is the primary goal of regularization in regression models?,To increase the number of parameters in the model,False,Regularization discourages large or unnecessary coefficients and can lead to sparsity.
What is the primary goal of regularization in regression models?,To increase the learning rate,False,Regularization and learning rate are independent hyperparameters with different purposes.
How does the bias-variance trade-off explain overfitting and underfitting?,Underfitting corresponds to low bias and high variance,False,Underfitting is usually the result of high bias and low variance.
How does the bias-variance trade-off explain overfitting and underfitting?,Overfitting corresponds to high bias and low variance,False,Overfitting occurs with low bias and high variance.
How does the bias-variance trade-off explain overfitting and underfitting?,Overfitting corresponds to low bias and high variance,True,Overfitting happens when a model fits the training data too closely due to low bias and high variance.
How does the bias-variance trade-off explain overfitting and underfitting?,Bias and variance have no impact on overfitting,False,They are the main theoretical framework for understanding overfitting and underfitting.
Why does Ridge regression not perform feature selection?,Because it applies an L2 penalty that cannot zero coefficients,True,L2 regularization only shrinks coefficients continuously without ever setting them to zero.
Why does Ridge regression not perform feature selection?,Because it removes outliers before training,False,Ridge does not remove data points; it regularizes coefficients.
Why does Ridge regression not perform feature selection?,Because it discards irrelevant variables based on correlation,False,Ridge includes all features but adjusts their weight to reduce overfitting.
Why does Ridge regression not perform feature selection?,Because it uses PCA before training,False,PCA is unrelated to Ridge regression unless explicitly used as preprocessing.
In what type of dataset is Ridge regression preferred over Lasso?,When many predictors are irrelevant,False,Lasso is better suited for selecting a few relevant variables in such scenarios.
In what type of dataset is Ridge regression preferred over Lasso?,When features are strongly collinear,True,Ridge can handle multicollinearity well by distributing weight among correlated variables.
In what type of dataset is Ridge regression preferred over Lasso?,When only one feature is significant,False,Lasso is more appropriate for sparse signal detection.
In what type of dataset is Ridge regression preferred over Lasso?,When there are fewer predictors than observations,False,While this can work with either method, collinearity is the stronger cue for Ridge.
What mathematical property of the L1 norm allows Lasso to set coefficients to exactly zero?,It uses a squared penalty function,False,That describes L2 regularization.
What mathematical property of the L1 norm allows Lasso to set coefficients to exactly zero?,Its constraint region has sharp corners,True,L1 norm creates a diamond-shaped constraint which often intersects the loss function at axes.
What mathematical property of the L1 norm allows Lasso to set coefficients to exactly zero?,It applies to interaction terms only,False,Lasso acts on all coefficients, not just interactions.
What mathematical property of the L1 norm allows Lasso to set coefficients to exactly zero?,It enforces equal weighting on all predictors,False,Lasso penalizes large coefficients, not equalizes them.
What does it mean for Lasso to create a sparse model?,It includes all features with small weights,False,That behavior is more typical of Ridge regression.
What does it mean for Lasso to create a sparse model?,It performs dimensionality reduction via SVD,False,Dimensionality reduction and Lasso are different techniques.
What does it mean for Lasso to create a sparse model?,It sets some feature coefficients to zero,True,Lasso eliminates irrelevant features by driving their coefficients to exactly zero.
What does it mean for Lasso to create a sparse model?,It multiplies all coefficients by zero,False,It selectively zeroes out only the least important features.
Why is Elastic Net considered more stable than Lasso alone?,It uses cross-validation,False,Cross-validation is a model selection technique, not a property of the regularization penalty.
Why is Elastic Net considered more stable than Lasso alone?,It only uses the L2 penalty,False,Elastic Net combines L1 and L2 penalties.
Why is Elastic Net considered more stable than Lasso alone?,It combines L1 and L2 which helps with correlated predictors,True,Elastic Net retains Lasso's sparsity while stabilizing weights across correlated features.
Why is Elastic Net considered more stable than Lasso alone?,It trains only on a subset of the data,False,Elastic Net trains on the full dataset unless subsampling is explicitly used.
How is computational complexity different between Ridge and Lasso?,Ridge is slower because it uses gradient descent,False,Ridge can be solved analytically in closed form, making it faster.
How is computational complexity different between Ridge and Lasso?,Lasso is slower due to iterative optimization,True,Lasso requires iterative solvers like coordinate descent since the objective is not differentiable.
How is computational complexity different between Ridge and Lasso?,Both use identical optimization algorithms,False,They use different solvers due to differences in their objective functions.
How is computational complexity different between Ridge and Lasso?,Lasso is always faster than Ridge,False,Lasso is often slower especially with many features.
Which regularization method is more suitable when the number of features exceeds the number of observations?,Ridge regression,False,Ridge includes all features which may not help when most are irrelevant.
Which regularization method is more suitable when the number of features exceeds the number of observations?,Lasso regression,True,Lasso performs variable selection which is ideal in high-dimensional sparse settings.
Which regularization method is more suitable when the number of features exceeds the number of observations?,OLS regression,False,OLS overfits and becomes unstable with more features than samples.
Which regularization method is more suitable when the number of features exceeds the number of observations?,None of them work,False,Both Ridge and Lasso were designed to deal with high-dimensionality.
Why does Ridge use a closed-form solution but Lasso does not?,Lasso is not convex,False,Lasso is convex but not differentiable due to the L1 penalty.
Why does Ridge use a closed-form solution but Lasso does not?,Lasso has no gradient,False,Lasso has subgradients but not a full gradient everywhere.
Why does Ridge use a closed-form solution but Lasso does not?,Lasso uses L1 norm which leads to non-differentiability,True,The absolute value in the L1 penalty makes the objective non-differentiable at zero.
Why does Ridge use a closed-form solution but Lasso does not?,Ridge is non-linear,False,Ridge is still a linear model with a smooth convex objective.
What would indicate that λ has been set too low in a Lasso model?,The model performs well on unseen data,False,That indicates good generalization and a proper lambda.
What would indicate that λ has been set too low in a Lasso model?,All coefficients are close to zero,False,That would happen with too high a lambda.
What would indicate that λ has been set too low in a Lasso model?,Most coefficients remain large and training error is low but test error is high,True,A small lambda fails to penalize complexity, leading to overfitting.
What would indicate that λ has been set too low in a Lasso model?,All variables are dropped,False,That would happen with an excessively high lambda.
When using Ridge or Lasso, why should the data often be standardized beforehand?,Because regularization depends on coefficient scale,True,Without standardization, features with large units dominate the penalty term.
When using Ridge or Lasso, why should the data often be standardized beforehand?,Because it helps PCA work better,False,PCA benefits from standardization but this is not related to Ridge or Lasso.
When using Ridge or Lasso, why should the data often be standardized beforehand?,Because it removes the need for regularization,False,Standardization helps fair penalization but does not replace regularization.
When using Ridge or Lasso, why should the data often be standardized beforehand?,Because it automatically selects optimal λ,False,Choosing lambda still requires cross-validation or tuning.
What is a key drawback of Lasso when predictors are highly correlated?,It tends to drop one feature arbitrarily,True,Lasso does not distribute weights evenly and may remove useful correlated features randomly.
What is a key drawback of Lasso when predictors are highly correlated?,It always keeps all correlated predictors,False,Lasso is known for eliminating redundant predictors.
What is a key drawback of Lasso when predictors are highly correlated?,It fails to shrink coefficients at all,False,Lasso shrinks and selects coefficients through its L1 penalty.
What is a key drawback of Lasso when predictors are highly correlated?,It cannot be used for regression problems,False,Lasso is explicitly designed for regression and classification problems.
How does Elastic Net address the limitations of Lasso?,By introducing an additional L2 penalty,True,Elastic Net adds L2 regularization which helps stabilize feature selection among correlated predictors.
How does Elastic Net address the limitations of Lasso?,By removing the penalty term altogether,False,Elastic Net still includes regularization through both L1 and L2 penalties.
How does Elastic Net address the limitations of Lasso?,By enforcing equal coefficients for all features,False,Elastic Net does not force equality but rather controls overfitting.
How does Elastic Net address the limitations of Lasso?,By using fewer training samples,False,Elastic Net does not reduce the data size but improves regularization on available data.
What is the effect of L2 regularization on correlated predictors?,It distributes weight among them,True,Ridge spreads influence among correlated features instead of dropping any single one.
What is the effect of L2 regularization on correlated predictors?,It drops all but one,False,That is a behavior associated with Lasso (L1) regularization.
What is the effect of L2 regularization on correlated predictors?,It assigns equal coefficients to all,False,Ridge adjusts them based on contribution, not equally.
What is the effect of L2 regularization on correlated predictors?,It zeroes all coefficients,False,Ridge shrinks but does not eliminate coefficients.
